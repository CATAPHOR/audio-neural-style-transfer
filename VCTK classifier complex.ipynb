{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2f1b35",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b11009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchaudio import datasets\n",
    "import torchaudio.transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "\n",
    "# audio processing\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# neural network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "#set device to GPU\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc34bc",
   "metadata": {},
   "source": [
    "## importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91988148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom VCTK-based dataset definition\n",
    "class STFTC_Dataset(Dataset):\n",
    "    def __init__(self, root: str):\n",
    "        self.root_path = root + \"/\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        # hard code as length will not change and to prevent needing to access original dataset\n",
    "        return 43873\n",
    "    \n",
    "    # load items from directory\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.root_path + \"stftc\" + str(idx), \"rb\") as file:\n",
    "            pickled_list = pickle.load(file)\n",
    "        \n",
    "        target_input, target_label = pickled_list\n",
    "        return torch.from_numpy(target_input).float(), target_label\n",
    "    \n",
    "# initialise dataset\n",
    "stftc_data = STFTC_Dataset(\"STFTcomp\")\n",
    "\n",
    "# split to test/train\n",
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * stftc_data.__len__())\n",
    "test_size = stftc_data.__len__() - train_size\n",
    "\n",
    "#device_gen = torch.Generator(device = device)\n",
    "train_dataset, test_dataset = random_split(stftc_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff0851",
   "metadata": {},
   "source": [
    "### dataset parameters\n",
    "\n",
    "*VCTK Structure:*\n",
    "(0: waveform; 1: sample rate; 2: text transcript; 3: person identifier; 4: text identifier)\n",
    "\n",
    "*STFT VCTK structure:*\n",
    "(0: waveform; 1: person identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80017cb7",
   "metadata": {},
   "source": [
    "### creating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670318f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate label dictionary from original VCTK dataset\n",
    "# label_temp_vctk = datasets.VCTK_092(root = \"VCTK\")\n",
    "\n",
    "# label_loader = iter(DataLoader(label_temp_vctk, shuffle = False))\n",
    "# dict_label = 0\n",
    "# label_dict = {}\n",
    "# while True:\n",
    "#    try:\n",
    "#        item = next(label_loader)\n",
    "#        if item[3] not in label_dict.keys():\n",
    "#            label_dict[item[3]] = dict_label\n",
    "#            dict_label = dict_label + 1\n",
    "#    except StopIteration:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62e9d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above code to generate, hard code the label dictionary once done:\n",
    "label_dict = {\n",
    "    ('p225',): 0, ('p226',): 1, ('p227',): 2, ('p228',): 3, ('p229',): 4, ('p230',): 5, ('p231',): 6, ('p232',): 7, \n",
    "    ('p233',): 8, ('p234',): 9, ('p236',): 10, ('p237',): 11, ('p238',): 12, ('p239',): 13, ('p240',): 14, ('p241',): 15, \n",
    "    ('p243',): 16, ('p244',): 17, ('p245',): 18, ('p246',): 19, ('p247',): 20, ('p248',): 21, ('p249',): 22, ('p250',): 23, \n",
    "    ('p251',): 24, ('p252',): 25, ('p253',): 26, ('p254',): 27, ('p255',): 28, ('p256',): 29, ('p257',): 30, ('p258',): 31, \n",
    "    ('p259',): 32, ('p260',): 33, ('p261',): 34, ('p262',): 35, ('p263',): 36, ('p264',): 37, ('p265',): 38, ('p266',): 39, \n",
    "    ('p267',): 40, ('p268',): 41, ('p269',): 42, ('p270',): 43, ('p271',): 44, ('p272',): 45, ('p273',): 46, ('p274',): 47, \n",
    "    ('p275',): 48, ('p276',): 49, ('p277',): 50, ('p278',): 51, ('p279',): 52, ('p281',): 53, ('p282',): 54, ('p283',): 55, \n",
    "    ('p284',): 56, ('p285',): 57, ('p286',): 58, ('p287',): 59, ('p288',): 60, ('p292',): 61, ('p293',): 62, ('p294',): 63, \n",
    "    ('p295',): 64, ('p297',): 65, ('p298',): 66, ('p299',): 67, ('p300',): 68, ('p301',): 69, ('p302',): 70, ('p303',): 71, \n",
    "    ('p304',): 72, ('p305',): 73, ('p306',): 74, ('p307',): 75, ('p308',): 76, ('p310',): 77, ('p311',): 78, ('p312',): 79, \n",
    "    ('p313',): 80, ('p314',): 81, ('p316',): 82, ('p317',): 83, ('p318',): 84, ('p323',): 85, ('p326',): 86, ('p329',): 87, \n",
    "    ('p330',): 88, ('p333',): 89, ('p334',): 90, ('p335',): 91, ('p336',): 92, ('p339',): 93, ('p340',): 94, ('p341',): 95, \n",
    "    ('p343',): 96, ('p345',): 97, ('p347',): 98, ('p351',): 99, ('p360',): 100, ('p361',): 101, ('p362',): 102, ('p363',): 103, \n",
    "    ('p364',): 104, ('p374',): 105, ('p376',): 106, ('s5',): 107\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9a59e",
   "metadata": {},
   "source": [
    "## defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a986771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to classify between the different speakers\n",
    "# we will treat the STFT matrix as if it is a 2d image: remember the columns are the audio signal windows with frequency components\n",
    "\n",
    "class VoiceCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VoiceCNN, self).__init__()\n",
    "        \n",
    "        #activation\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        #CONV LAYERS\n",
    "        \n",
    "        #interpret complex numbers as two channels\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 8, kernel_size = (5, 5), stride = 2, padding = 1)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, mode = 'fan_in', nonlinearity = 'relu') # initialise weights\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3), stride = 2, padding = 1)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, mode = 'fan_in', nonlinearity = 'relu')\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3, 3), stride = 2, padding = 1)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, mode = 'fan_in', nonlinearity = 'relu')\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (3, 3), stride = 2, padding = 1)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight, mode = 'fan_in', nonlinearity = 'relu')\n",
    "        \n",
    "        #POOLING\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        pool_h, pool_w = 2, 2\n",
    "        self.adaptive_pooling = nn.AdaptiveAvgPool2d((pool_h, pool_w))\n",
    "        \n",
    "        # define conv block\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.activation,\n",
    "            self.pool,\n",
    "            self.conv2,\n",
    "            self.activation,\n",
    "            self.pool,\n",
    "            self.conv3,\n",
    "            self.activation,\n",
    "            self.pool,\n",
    "            self.conv4,\n",
    "            self.activation,\n",
    "            self.pool\n",
    "        )\n",
    "        \n",
    "        #FULLY CONNECTED LAYERS\n",
    "        \n",
    "        self.fc1 = nn.Linear(pool_h * pool_w * self.conv4.out_channels, 256)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode = 'fan_in', nonlinearity = 'relu')\n",
    "        self.fc2 = nn.Linear(256, 108)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode = 'fan_in', nonlinearity = 'relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # through conv layers\n",
    "        out = self.conv_layers(x)\n",
    "        \n",
    "        # adaptive pooling to standardise feature map size\n",
    "        out = self.adaptive_pooling(out)\n",
    "        \n",
    "        # flatten feature maps\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # through connected layers\n",
    "        out = self.activation(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45748be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 257, 1663])\n",
      "torch.Size([1, 108])\n"
     ]
    }
   ],
   "source": [
    "# sanity test model with forward pass\n",
    "model = VoiceCNN().to(device)\n",
    "\n",
    "def test_forward(x):\n",
    "    example = x[0][None, :, :, :].to(device)\n",
    "    print(example.size())\n",
    "    output = model(example)\n",
    "    return output\n",
    "    \n",
    "output = test_forward(test_dataset.__getitem__(0))\n",
    "print(nn.Softmax(dim = 1)(output).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c8af9",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "415858ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1663\n"
     ]
    }
   ],
   "source": [
    "test = test_dataset.__getitem__(0)\n",
    "print(test[0].shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d760b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparam\n",
    "set_batch_size = 32\n",
    "\n",
    "# custom collate function as input STFTs are of differing lengths\n",
    "def collate_pad(batch):\n",
    "    # sort by length in descending order\n",
    "    batch.sort(key = lambda x: x[0].shape[2], reverse = True)\n",
    "    max_length = batch[0][0].shape[2]\n",
    "    \n",
    "    # pad all inputs to same length\n",
    "    padded_X = []\n",
    "    for item in batch:\n",
    "        tensor, label = item\n",
    "        pad_size = max_length - tensor.shape[2]\n",
    "        padded = F.pad(tensor, (0, pad_size))\n",
    "        padded_X.append(padded)\n",
    "    \n",
    "    # return padded X in a stack\n",
    "    X = torch.stack(padded_X)\n",
    "    \n",
    "    # convert labels to one-hot stack\n",
    "    dict_labels = torch.tensor([label_dict[(item[1],)] for item in batch])\n",
    "    y = F.one_hot(dict_labels, len(label_dict)).float()\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = set_batch_size, shuffle = True, collate_fn = collate_pad)\n",
    "test_loader = DataLoader(test_dataset, batch_size = set_batch_size, shuffle = False, collate_fn = collate_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb65067",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1adc3d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([32, 2, 257, 3786])\n",
      "Shape of y: torch.Size([32, 108]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# def training func\n",
    "def train(dataloader, model, loss_fn, optimiser):\n",
    "    length = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # pred error\n",
    "        hx = model(X)\n",
    "        loss = loss_fn(hx, y)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{length:>5d}]\")\n",
    "\n",
    "# def test func\n",
    "def test(dataloader, model, loss_fn):\n",
    "    length = len(dataloader.dataset)\n",
    "    batch_length = len(dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, total_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            hx = model(X)\n",
    "            test_loss += loss_fn(hx, y).item()\n",
    "            \n",
    "            total_correct += (hx.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "        test_loss /= batch_length\n",
    "        total_correct /= length\n",
    "        print(f\"Test Error: \\n Accuracy: {(100 * total_correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# sanity test\n",
    "\n",
    "for X, y in test_loader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0870f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:\n",
      "loss: 0.171932  [   32/35098]\n",
      "loss: 0.864804  [ 3232/35098]\n",
      "loss: 0.767665  [ 6432/35098]\n",
      "loss: 0.341065  [ 9632/35098]\n",
      "loss: 0.273837  [12832/35098]\n",
      "loss: 0.245040  [16032/35098]\n",
      "loss: 0.255948  [19232/35098]\n",
      "loss: 0.447391  [22432/35098]\n",
      "loss: 0.387948  [25632/35098]\n",
      "loss: 0.349256  [28832/35098]\n",
      "loss: 0.376904  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.462848 \n",
      "\n",
      "\n",
      "Epoch 1:\n",
      "loss: 0.190572  [   32/35098]\n",
      "loss: 0.247172  [ 3232/35098]\n",
      "loss: 0.213811  [ 6432/35098]\n",
      "loss: 0.289042  [ 9632/35098]\n",
      "loss: 0.163893  [12832/35098]\n",
      "loss: 0.269561  [16032/35098]\n",
      "loss: 0.278807  [19232/35098]\n",
      "loss: 0.225892  [22432/35098]\n",
      "loss: 0.442743  [25632/35098]\n",
      "loss: 0.155530  [28832/35098]\n",
      "loss: 0.405546  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.441864 \n",
      "\n",
      "\n",
      "Epoch 2:\n",
      "loss: 0.260344  [   32/35098]\n",
      "loss: 0.147010  [ 3232/35098]\n",
      "loss: 0.043669  [ 6432/35098]\n",
      "loss: 0.079303  [ 9632/35098]\n",
      "loss: 0.293366  [12832/35098]\n",
      "loss: 0.123277  [16032/35098]\n",
      "loss: 0.040464  [19232/35098]\n",
      "loss: 0.074694  [22432/35098]\n",
      "loss: 0.204868  [25632/35098]\n",
      "loss: 0.469014  [28832/35098]\n",
      "loss: 0.410646  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.329428 \n",
      "\n",
      "\n",
      "Epoch 3:\n",
      "loss: 0.124110  [   32/35098]\n",
      "loss: 0.140698  [ 3232/35098]\n",
      "loss: 0.089150  [ 6432/35098]\n",
      "loss: 0.051721  [ 9632/35098]\n",
      "loss: 0.239793  [12832/35098]\n",
      "loss: 0.088695  [16032/35098]\n",
      "loss: 0.518129  [19232/35098]\n",
      "loss: 0.073184  [22432/35098]\n",
      "loss: 0.378040  [25632/35098]\n",
      "loss: 0.063579  [28832/35098]\n",
      "loss: 0.112172  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.327231 \n",
      "\n",
      "\n",
      "Epoch 4:\n",
      "loss: 0.059045  [   32/35098]\n",
      "loss: 0.194458  [ 3232/35098]\n",
      "loss: 0.122812  [ 6432/35098]\n",
      "loss: 0.167095  [ 9632/35098]\n",
      "loss: 0.095671  [12832/35098]\n",
      "loss: 0.056747  [16032/35098]\n",
      "loss: 0.071057  [19232/35098]\n",
      "loss: 0.116496  [22432/35098]\n",
      "loss: 0.105806  [25632/35098]\n",
      "loss: 0.060436  [28832/35098]\n",
      "loss: 0.033700  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.320920 \n",
      "\n",
      "\n",
      "Epoch 5:\n",
      "loss: 0.049054  [   32/35098]\n",
      "loss: 0.044189  [ 3232/35098]\n",
      "loss: 0.324905  [ 6432/35098]\n",
      "loss: 0.065156  [ 9632/35098]\n",
      "loss: 0.324975  [12832/35098]\n",
      "loss: 0.034988  [16032/35098]\n",
      "loss: 0.376625  [19232/35098]\n",
      "loss: 0.133920  [22432/35098]\n",
      "loss: 0.156443  [25632/35098]\n",
      "loss: 0.076614  [28832/35098]\n",
      "loss: 0.043418  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.313472 \n",
      "\n",
      "\n",
      "Epoch 6:\n",
      "loss: 0.064789  [   32/35098]\n",
      "loss: 0.043908  [ 3232/35098]\n",
      "loss: 0.079080  [ 6432/35098]\n",
      "loss: 0.097967  [ 9632/35098]\n",
      "loss: 0.095977  [12832/35098]\n",
      "loss: 0.101201  [16032/35098]\n",
      "loss: 0.050455  [19232/35098]\n",
      "loss: 0.009205  [22432/35098]\n",
      "loss: 0.171665  [25632/35098]\n",
      "loss: 0.072175  [28832/35098]\n",
      "loss: 0.064777  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.280167 \n",
      "\n",
      "\n",
      "Epoch 7:\n",
      "loss: 0.121380  [   32/35098]\n",
      "loss: 0.100038  [ 3232/35098]\n",
      "loss: 0.053585  [ 6432/35098]\n",
      "loss: 0.064831  [ 9632/35098]\n",
      "loss: 0.017189  [12832/35098]\n",
      "loss: 0.035993  [16032/35098]\n",
      "loss: 0.007201  [19232/35098]\n",
      "loss: 0.275467  [22432/35098]\n",
      "loss: 0.154035  [25632/35098]\n",
      "loss: 0.330044  [28832/35098]\n",
      "loss: 0.027948  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.308213 \n",
      "\n",
      "\n",
      "Epoch 8:\n",
      "loss: 0.128332  [   32/35098]\n",
      "loss: 0.150948  [ 3232/35098]\n",
      "loss: 0.082132  [ 6432/35098]\n",
      "loss: 0.044512  [ 9632/35098]\n",
      "loss: 0.007346  [12832/35098]\n",
      "loss: 0.051002  [16032/35098]\n",
      "loss: 0.028699  [19232/35098]\n",
      "loss: 0.005830  [22432/35098]\n",
      "loss: 0.105635  [25632/35098]\n",
      "loss: 0.178368  [28832/35098]\n",
      "loss: 0.103837  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.297419 \n",
      "\n",
      "\n",
      "Epoch 9:\n",
      "loss: 0.092060  [   32/35098]\n",
      "loss: 0.123048  [ 3232/35098]\n",
      "loss: 0.053325  [ 6432/35098]\n",
      "loss: 0.025250  [ 9632/35098]\n",
      "loss: 0.188893  [12832/35098]\n",
      "loss: 0.007521  [16032/35098]\n",
      "loss: 0.034638  [19232/35098]\n",
      "loss: 0.021064  [22432/35098]\n",
      "loss: 0.076027  [25632/35098]\n",
      "loss: 0.117988  [28832/35098]\n",
      "loss: 0.014459  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.295943 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "learning_rate = 0.002\n",
    "weight_decay = 1e-6\n",
    "epochs = 10\n",
    "\n",
    "# loss and optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimiser, gamma = 0.8)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nEpoch \" + str(epoch) +\":\")\n",
    "    train(train_loader, model, loss_fn, optimiser)\n",
    "    scheduler.step()\n",
    "    test(test_loader, model, loss_fn)\n",
    "    torch.save(model.state_dict(), \"comp_model_params\" + \"_ep_\" + str(epoch) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b2c4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"comp_model_params.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b67f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:\n",
      "loss: 0.105944  [   32/35098]\n",
      "loss: 0.019909  [ 3232/35098]\n",
      "loss: 0.052351  [ 6432/35098]\n",
      "loss: 0.003501  [ 9632/35098]\n",
      "loss: 0.074927  [12832/35098]\n",
      "loss: 0.027165  [16032/35098]\n",
      "loss: 0.115724  [19232/35098]\n",
      "loss: 0.048907  [22432/35098]\n",
      "loss: 0.053381  [25632/35098]\n",
      "loss: 0.013843  [28832/35098]\n",
      "loss: 0.001674  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.293395 \n",
      "\n",
      "\n",
      "Epoch 1:\n",
      "loss: 0.052659  [   32/35098]\n",
      "loss: 0.022348  [ 3232/35098]\n",
      "loss: 0.152860  [ 6432/35098]\n",
      "loss: 0.082621  [ 9632/35098]\n",
      "loss: 0.048076  [12832/35098]\n",
      "loss: 0.062924  [16032/35098]\n",
      "loss: 0.176050  [19232/35098]\n",
      "loss: 0.050535  [22432/35098]\n",
      "loss: 0.013892  [25632/35098]\n",
      "loss: 0.022687  [28832/35098]\n",
      "loss: 0.125794  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.291956 \n",
      "\n",
      "\n",
      "Epoch 2:\n",
      "loss: 0.029710  [   32/35098]\n",
      "loss: 0.031510  [ 3232/35098]\n",
      "loss: 0.008850  [ 6432/35098]\n",
      "loss: 0.004483  [ 9632/35098]\n",
      "loss: 0.027906  [12832/35098]\n",
      "loss: 0.012686  [16032/35098]\n",
      "loss: 0.028374  [19232/35098]\n",
      "loss: 0.038657  [22432/35098]\n",
      "loss: 0.003448  [25632/35098]\n",
      "loss: 0.050730  [28832/35098]\n",
      "loss: 0.022994  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.287154 \n",
      "\n",
      "\n",
      "Epoch 3:\n",
      "loss: 0.070558  [   32/35098]\n",
      "loss: 0.028941  [ 3232/35098]\n",
      "loss: 0.008457  [ 6432/35098]\n",
      "loss: 0.014082  [ 9632/35098]\n",
      "loss: 0.005500  [12832/35098]\n",
      "loss: 0.017214  [16032/35098]\n",
      "loss: 0.012678  [19232/35098]\n",
      "loss: 0.019409  [22432/35098]\n",
      "loss: 0.075474  [25632/35098]\n",
      "loss: 0.012724  [28832/35098]\n",
      "loss: 0.004438  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 93.1%, Avg loss: 0.293157 \n",
      "\n",
      "\n",
      "Epoch 4:\n",
      "loss: 0.062491  [   32/35098]\n",
      "loss: 0.020385  [ 3232/35098]\n",
      "loss: 0.003935  [ 6432/35098]\n",
      "loss: 0.021902  [ 9632/35098]\n",
      "loss: 0.035083  [12832/35098]\n",
      "loss: 0.123692  [16032/35098]\n",
      "loss: 0.007550  [19232/35098]\n",
      "loss: 0.047618  [22432/35098]\n",
      "loss: 0.006724  [25632/35098]\n",
      "loss: 0.055898  [28832/35098]\n",
      "loss: 0.008546  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.289293 \n",
      "\n",
      "\n",
      "Epoch 5:\n",
      "loss: 0.067786  [   32/35098]\n",
      "loss: 0.017608  [ 3232/35098]\n",
      "loss: 0.077876  [ 6432/35098]\n",
      "loss: 0.061515  [ 9632/35098]\n",
      "loss: 0.023797  [12832/35098]\n",
      "loss: 0.009695  [16032/35098]\n",
      "loss: 0.005318  [19232/35098]\n",
      "loss: 0.050527  [22432/35098]\n",
      "loss: 0.015820  [25632/35098]\n",
      "loss: 0.166583  [28832/35098]\n",
      "loss: 0.011643  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.298565 \n",
      "\n",
      "\n",
      "Epoch 6:\n",
      "loss: 0.029363  [   32/35098]\n",
      "loss: 0.058806  [ 3232/35098]\n",
      "loss: 0.043957  [ 6432/35098]\n",
      "loss: 0.022416  [ 9632/35098]\n",
      "loss: 0.014524  [12832/35098]\n",
      "loss: 0.002218  [16032/35098]\n",
      "loss: 0.007169  [19232/35098]\n",
      "loss: 0.006333  [22432/35098]\n",
      "loss: 0.011400  [25632/35098]\n",
      "loss: 0.032051  [28832/35098]\n",
      "loss: 0.027911  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.293709 \n",
      "\n",
      "\n",
      "Epoch 7:\n",
      "loss: 0.002771  [   32/35098]\n",
      "loss: 0.066559  [ 3232/35098]\n",
      "loss: 0.008349  [ 6432/35098]\n",
      "loss: 0.048847  [ 9632/35098]\n",
      "loss: 0.011115  [12832/35098]\n",
      "loss: 0.014608  [16032/35098]\n",
      "loss: 0.022997  [19232/35098]\n",
      "loss: 0.022620  [22432/35098]\n",
      "loss: 0.005843  [25632/35098]\n",
      "loss: 0.112849  [28832/35098]\n",
      "loss: 0.082574  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.294488 \n",
      "\n",
      "\n",
      "Epoch 8:\n",
      "loss: 0.033550  [   32/35098]\n",
      "loss: 0.041665  [ 3232/35098]\n",
      "loss: 0.012115  [ 6432/35098]\n",
      "loss: 0.002168  [ 9632/35098]\n",
      "loss: 0.019026  [12832/35098]\n",
      "loss: 0.053616  [16032/35098]\n",
      "loss: 0.017023  [19232/35098]\n",
      "loss: 0.007419  [22432/35098]\n",
      "loss: 0.051184  [25632/35098]\n",
      "loss: 0.001741  [28832/35098]\n",
      "loss: 0.001942  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.293828 \n",
      "\n",
      "\n",
      "Epoch 9:\n",
      "loss: 0.032753  [   32/35098]\n",
      "loss: 0.025257  [ 3232/35098]\n",
      "loss: 0.007078  [ 6432/35098]\n",
      "loss: 0.016221  [ 9632/35098]\n",
      "loss: 0.035472  [12832/35098]\n",
      "loss: 0.038215  [16032/35098]\n",
      "loss: 0.005556  [19232/35098]\n",
      "loss: 0.015958  [22432/35098]\n",
      "loss: 0.025588  [25632/35098]\n",
      "loss: 0.004380  [28832/35098]\n",
      "loss: 0.022632  [32032/35098]\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.293831 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"\\nEpoch \" + str(epoch) +\":\")\n",
    "    train(train_loader, model, loss_fn, optimiser)\n",
    "    scheduler.step()\n",
    "    test(test_loader, model, loss_fn)\n",
    "    torch.save(model.state_dict(), \"comp_model_params_further\" + \"_ep_\" + str(epoch) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1e84d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
